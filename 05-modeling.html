<!DOCTYPE html>
<html lang="en"><head>
<script src="05-modeling_files/libs/quarto-html/tabby.min.js"></script>
<script src="05-modeling_files/libs/quarto-html/popper.min.js"></script>
<script src="05-modeling_files/libs/quarto-html/tippy.umd.min.js"></script>
<link href="05-modeling_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="05-modeling_files/libs/quarto-html/light-border.css" rel="stylesheet">
<link href="05-modeling_files/libs/quarto-html/quarto-html.min.css" rel="stylesheet" data-mode="light">
<link href="05-modeling_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.4.549">

  <meta name="author" content="Matteo Francia   DISI — University of Bologna   m.francia@unibo.it">
  <title>Machine Learning and Data Mining (Module 2)</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="05-modeling_files/libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="05-modeling_files/libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="05-modeling_files/libs/revealjs/dist/theme/quarto.css">
  <link href="05-modeling_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="05-modeling_files/libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="05-modeling_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="05-modeling_files/libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">

  .callout {
    margin-top: 1em;
    margin-bottom: 1em;  
    border-radius: .25rem;
  }

  .callout.callout-style-simple { 
    padding: 0em 0.5em;
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
    display: flex;
  }

  .callout.callout-style-default {
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
  }

  .callout .callout-body-container {
    flex-grow: 1;
  }

  .callout.callout-style-simple .callout-body {
    font-size: 1rem;
    font-weight: 400;
  }

  .callout.callout-style-default .callout-body {
    font-size: 0.9rem;
    font-weight: 400;
  }

  .callout.callout-titled.callout-style-simple .callout-body {
    margin-top: 0.2em;
  }

  .callout:not(.callout-titled) .callout-body {
      display: flex;
  }

  .callout:not(.no-icon).callout-titled.callout-style-simple .callout-content {
    padding-left: 1.6em;
  }

  .callout.callout-titled .callout-header {
    padding-top: 0.2em;
    margin-bottom: -0.2em;
  }

  .callout.callout-titled .callout-title  p {
    margin-top: 0.5em;
    margin-bottom: 0.5em;
  }
    
  .callout.callout-titled.callout-style-simple .callout-content  p {
    margin-top: 0;
  }

  .callout.callout-titled.callout-style-default .callout-content  p {
    margin-top: 0.7em;
  }

  .callout.callout-style-simple div.callout-title {
    border-bottom: none;
    font-size: .9rem;
    font-weight: 600;
    opacity: 75%;
  }

  .callout.callout-style-default  div.callout-title {
    border-bottom: none;
    font-weight: 600;
    opacity: 85%;
    font-size: 0.9rem;
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-default div.callout-content {
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-simple .callout-icon::before {
    height: 1rem;
    width: 1rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 1rem 1rem;
  }

  .callout.callout-style-default .callout-icon::before {
    height: 0.9rem;
    width: 0.9rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 0.9rem 0.9rem;
  }

  .callout-title {
    display: flex
  }
    
  .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  .callout.no-icon::before {
    display: none !important;
  }

  .callout.callout-titled .callout-body > .callout-content > :last-child {
    padding-bottom: 0.5rem;
    margin-bottom: 0;
  }

  .callout.callout-titled .callout-icon::before {
    margin-top: .5rem;
    padding-right: .5rem;
  }

  .callout:not(.callout-titled) .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  /* Callout Types */

  div.callout-note {
    border-left-color: #4582ec !important;
  }

  div.callout-note .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEU0lEQVRYCcVXTWhcVRQ+586kSUMMxkyaElstCto2SIhitS5Ek8xUKV2poatCcVHtUlFQk8mbaaziwpWgglJwVaquitBOfhQXFlqlzSJpFSpIYyXNjBNiTCck7x2/8/LeNDOZxDuEkgOXe++553zfefee+/OYLOXFk3+1LLrRdiO81yNqZ6K9cG0P3MeFaMIQjXssE8Z1JzLO9ls20MBZX7oG8w9GxB0goaPrW5aNMp1yOZIa7Wv6o2ykpLtmAPs/vrG14Z+6d4jpbSKuhdcSyq9wGMPXjonwmESXrriLzFGOdDBLB8Y6MNYBu0dRokSygMA/mrun8MGFN3behm6VVAwg4WR3i6FvYK1T7MHo9BK7ydH+1uurECoouk5MPRyVSBrBHMYwVobG2aOXM07sWrn5qgB60rc6mcwIDJtQrnrEr44kmy+UO9r0u9O5/YbkS9juQckLed3DyW2XV/qWBBB3ptvI8EUY3I9p/67OW+g967TNr3Sotn3IuVlfMLVnsBwH4fsnebJvyGm5GeIUA3jljERmrv49SizPYuq+z7c2H/jlGC+Ghhupn/hcapqmcudB9jwJ/3jvnvu6vu5lVzF1fXyZuZZ7U8nRmVzytvT+H3kilYvH09mLWrQdwFSsFEsxFVs5fK7A0g8gMZjbif4ACpKbjv7gNGaD8bUrlk8x+KRflttr22JEMRUbTUwwDQScyzPgedQHZT0xnx7ujw2jfVfExwYHwOsDTjLdJ2ebmeQIlJ7neo41s/DrsL3kl+W2lWvAga0tR3zueGr6GL78M3ifH0rGXrBC2aAR8uYcIA5gwV8zIE8onoh8u0Fca/ciF7j1uOzEnqcIm59sEXoGc0+z6+H45V1CvAvHcD7THztu669cnp+L0okAeIc6zjbM/24LgGM1gZk7jnRu1aQWoU9sfUOuhrmtaPIO3YY1KLLWZaEO5TKUbMY5zx8W9UJ6elpLwKXbsaZ4EFl7B4bMtDv0iRipKoDQT2sNQI9b1utXFdYisi+wzZ/ri/1m7QfDgEuvgUUEIJPq3DhX/5DWNqIXDOweC2wvIR90Oq3lDpdMIgD2r0dXvGdsEW5H6x6HLRJYU7C69VefO1x8Gde1ZFSJLfWS1jbCnhtOPxmpfv2LXOA2Xk2tvnwKKPFuZ/oRmwBwqRQDcKNeVQkYcOjtWVBuM/JuYw5b6isojIkYxyYAFn5K7ZBF10fea52y8QltAg6jnMqNHFBmGkQ1j+U43HMi2xMar1Nv0zGsf1s8nUsmUtPOOrbFIR8bHFDMB5zL13Gmr/kGlCkUzedTzzmzsaJXhYawnA3UmARpiYj5ooJZiUoxFRtK3X6pgNPv+IZVPcnwbOl6f+aBaO1CNvPW9n9LmCp01nuSaTRF2YxHqZ8DYQT6WsXT+RD6eUztwYLZ8rM+rcPxamv1VQzFUkzFXvkiVrySGQgJNvXHJAxiU3/NwiC03rSf05VBaPtu/Z7/B8Yn/w7eguloAAAAAElFTkSuQmCC');
  }

  div.callout-note.callout-style-default .callout-title {
    background-color: #dae6fb
  }

  div.callout-important {
    border-left-color: #d9534f !important;
  }

  div.callout-important .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEKklEQVRYCcVXTWhcVRS+575MJym48A+hSRFr00ySRQhURRfd2HYjk2SSTokuBCkU2o0LoSKKraKIBTcuFCoidGFD08nkBzdREbpQ1EDNIv8qSGMFUboImMSZd4/f9zJv8ibJMC8xJQfO3HPPPef7zrvvvnvviIkpC9nsw0UttFunbUhpFzFtarSd6WJkStVMw5xyVqYTvkwfzuf/5FgtkVoB0729j1rjXwThS7Vio+Mo6DNnvLfahoZ+i/o32lULuJ3NNiz7q6+pyAUkJaFF6JwaM2lUJlV0MlnQn5aTRbEu0SEqHUa0A4AdiGuB1kFXRfVyg5d87+Dg4DL6m2TLAub60ilj7A1Ec4odSAc8X95sHh7+ZRPCFo6Fnp7HfU/fBng/hi10CjCnWnJjsxvDNxWw0NfV6Rv5GgP3I3jGWXumdTD/3cbEOP2ZbOZp69yniG3FQ9z1jD7bnBu9Fc2tKGC2q+uAJOQHBDRiZX1x36o7fWBs7J9ownbtO+n0/qWkvW7UPIfc37WgT6ZGR++EOJyeQDSb9UB+DZ1G6DdLDzyS+b/kBCYGsYgJbSQHuThGKRcw5xdeQf8YdNHsc6ePXrlSYMBuSIAFTGAtQo+VuALo4BX83N190NWZWbynBjhOHsmNfFWLeL6v+ynsA58zDvvAC8j5PkbOcXCMg2PZFk3q8MjI7WAG/Dp9AwP7jdGBOOQkAvlFUB+irtm16I1Zw9YBcpGTGXYmk3kQIC/Cds55l+iMI3jqhjAuaoe+am2Jw5GT3Nbz3CkE12NavmzN5+erJW7046n/CH1RO/RVa8lBLozXk9uqykkGAyRXLWlLv5jyp4RFsG5vGVzpDLnIjTWgnRy2Rr+tDKvRc7Y8AyZq10jj8DqXdnIRNtFZb+t/ZRtXcDiVnzpqx8mPcDWxgARUqx0W1QB9MeUZiNrV4qP+Ehc+BpNgATsTX8ozYKL2NtFYAHc84fG7ndxUPr+AR/iQSns7uSUufAymwDOb2+NjK27lEFocm/EE2WpyIy/Hi66MWuMKJn8RvxIcj87IM5Vh9663ziW36kR0HNenXuxmfaD8JC7tfKbrhFr7LiZCrMjrzTeGx+PmkosrkNzW94ObzwocJ7A1HokLolY+AvkTiD/q1H0cN48c5EL8Crkttsa/AXQVDmutfyku0E7jShx49XqV3MFK8IryDhYVbj7Sj2P2eBxwcXoe8T8idsKKPRcnZw1b+slFTubwUwhktrfnAt7J++jwQtLZcm3sr9LQrjRzz6cfMv9aLvgmnAGvpoaGLxM4mAEaLV7iAzQ3oU0IvD5x9ix3yF2RAAuYAOO2f7PEFWCXZ4C9Pb2UsgDeVnFSpbFK7/IWu7TPTvBqzbGdCHOJQSxiEjt6IyZmxQyEJHv6xyQsYk//moVFsN2zP6fRImjfq7/n/wFDguUQFNEwugAAAABJRU5ErkJggg==');
  }

  div.callout-important.callout-style-default .callout-title {
    background-color: #f7dddc
  }

  div.callout-warning {
    border-left-color: #f0ad4e !important;
  }

  div.callout-warning .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAETklEQVRYCeVWW2gcVRg+58yaTUnizqbipZeX4uWhBEniBaoUX1Ioze52t7sRq6APio9V9MEaoWlVsFasRq0gltaAPuxms8lu0gcviE/FFOstVbSIxgcv6SU7EZqmdc7v9+9mJtNks51NTUH84ed889/PP+cmxP+d5FIbMJmNbpREu4WUkiTtCicKny0l1pIKmBzovF2S+hIJHX8iEu3hZJ5lNZGqyRrGSIQpq15AzF28jgpeY6yk6GVdrfFqdrD6Iw+QlB8g0YS2g7dyQmXM/IDhBhT0UCiRf59lfqmmDvzRt6kByV/m4JjtzuaujMUM2c5Z2d6JdKrRb3K2q6mA+oYVz8JnDdKPmmNthzkAk/lN63sYPgevrguc72aZX/L9C6x09GYyxBgCX4NlvyGUHOKELlm5rXeR1kchuChJt4SSwyddZRXgvwMGvYo4QSlk3/zkHD8UHxwVJA6zjZZqP8v8kK8OWLnIZtLyCAJagYC4rTGW/9Pqj92N/c+LUaAj27movwbi19tk/whRCIE7Q9vyI6yvRpftAKVTdUjOW40X3h5OXsKCdmFcx0xlLJoSuQngnrJe7Kcjm4OMq9FlC7CMmScQANuNvjfP3PjGXDBaUQmbp296S5L4DrpbrHN1T87ZVEZVCzg1FF0Ft+dKrlLukI+/c9ENo+TvlTDbYFvuKPtQ9+l052rXrgKoWkDAFnvh0wTOmYn8R5f4k/jN/fZiCM1tQx9jQQ4ANhqG4hiL0qIFTGViG9DKB7GYzgubnpofgYRwO+DFjh0Zin2m4b/97EDkXkc+f6xYAPX0KK2I/7fUQuwzuwo/L3AkcjugPNixC8cHf0FyPjWlItmLxWw4Ou9YsQCr5fijMGoD/zpdRy95HRysyXA74MWOnscpO4j2y3HAVisw85hX5+AFBRSHt4ShfLFkIMXTqyKFc46xdzQM6XbAi702a7sy04J0+feReMFKp5q9esYLCqAZYw/k14E/xcLLsFElaornTuJB0svMuJINy8xkIYuL+xPAlWRceH6+HX7THJ0djLUom46zREu7tTkxwmf/FdOZ/sh6Q8qvEAiHpm4PJ4a/doJe0gH1t+aHRgCzOvBvJedEK5OFE5jpm4AGP2a8Dxe3gGJ/pAutug9Gp6he92CsSsWBaEcxGx0FHytmIpuqGkOpldqNYQK8cSoXvd+xLxXADw0kf6UkJNFtdo5MOgaLjiQOQHcn+A6h5NuL2s0qsC2LOM75PcF3yr5STuBSAcGG+meA14K/CI21HcS4LBT6tv0QAh8Dr5l93AhZzG5ZJ4VxAqdZUEl9z7WJ4aN+svMvwHHL21UKTd1mqvChH7/Za5xzXBBKrUcB0TQ+Ulgkfbi/H/YT5EptrGzsEK7tR1B7ln9BBwckYfMiuSqklSznIuoIIOM42MQO+QnduCoFCI0bpkzjCjddHPN/F+2Yu+sd9bKNpVwHhbS3LluK/0zgfwD0xYI5dXuzlQAAAABJRU5ErkJggg==');
  }

  div.callout-warning.callout-style-default .callout-title {
    background-color: #fcefdc
  }

  div.callout-tip {
    border-left-color: #02b875 !important;
  }

  div.callout-tip .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAADr0lEQVRYCe1XTWgTQRj9ZjZV8a9SPIkKgj8I1bMHsUWrqYLVg4Ue6v9BwZOxSYsIerFao7UiUryIqJcqgtpimhbBXoSCVxUFe9CTiogUrUp2Pt+3aUI2u5vdNh4dmMzOzHvvezuz8xNFM0mjnbXaNu1MvFWRXkXEyE6aYOYJpdW4IXuA4r0fo8qqSMDBU0v1HJUgVieAXxzCsdE/YJTdFcVIZQNMyhruOMJKXYFoLfIfIvVIMWdsrd+Rpd86ZmyzzjJmLStqRn0v8lzkb4rVIXvnpScOJuAn2ACC65FkPzEdEy4TPWRLJ2h7z4cArXzzaOdKlbOvKKX25Wl00jSnrwVxAg3o4dRxhO13RBSdNvH0xSARv3adTXbBdTf64IWO2vH0LT+cv4GR1DJt+DUItaQogeBX/chhbTBxEiZ6gftlDNXTrvT7co4ub5A6gp9HIcHvzTa46OS5fBeP87Qm0fQkr4FsYgVQ7Qg+ZayaDg9jhg1GkWj8RG6lkeSacrrHgDaxdoBiZPg+NXV/KifMuB6//JmYH4CntVEHy/keA6x4h4CU5oFy8GzrBS18cLJMXcljAKB6INjWsRcuZBWVaS3GDrqB7rdapVIeA+isQ57Eev9eCqzqOa81CY05VLd6SamW2wA2H3SiTbnbSxmzfp7WtKZkqy4mdyAlGx7ennghYf8voqp9cLSgKdqNfa6RdRsAAkPwRuJZNbpByn+RrJi1RXTwdi8RQF6ymDwGMAtZ6TVE+4uoKh+MYkcLsT0Hk8eAienbiGdjJHZTpmNjlbFJNKDVAp2fJlYju6IreQxQ08UJDNYdoLSl6AadO+fFuCQqVMB1NJwPm69T04Wv5WhfcWyfXQB+wXRs1pt+nCknRa0LVzSA/2B+a9+zQJadb7IyyV24YAxKp2Jqs3emZTuNnKxsah+uabKbMk7CbTgJx/zIgQYErIeTKRQ9yD9wxVof5YolPHqaWo7TD6tJlh7jQnK5z2n3+fGdggIOx2kaa2YI9QWarc5Ce1ipNWMKeSG4DysFF52KBmTNMmn5HqCFkwy34rDg05gDwgH3bBi+sgFhN/e8QvRn8kbamCOhgrZ9GJhFDgfcMHzFb6BAtjKpFhzTjwv1KCVuxHvCbsSiEz4CANnj84cwHdFXAbAOJ4LTSAawGWFn5tDhLMYz6nWeU2wJfIhmIJBefcd/A5FWQWGgrWzyORZ3Q6HuV+Jf0Bj+BTX69fm1zWgK7By1YTXchFDORywnfQ7GpzOo6S+qECrsx2ifVQAAAABJRU5ErkJggg==');
  }

  div.callout-tip.callout-style-default .callout-title {
    background-color: #ccf1e3
  }

  div.callout-caution {
    border-left-color: #fd7e14 !important;
  }

  div.callout-caution .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAACV0lEQVRYCdVWzWoUQRCuqp2ICBLJXgITZL1EfQDBW/bkzUMUD7klD+ATSHBEfAIfQO+iXsWDxJsHL96EHAwhgzlkg8nBg25XWb0zIb0zs9muYYWkoKeru+vn664fBqElyZNuyh167NXJ8Ut8McjbmEraKHkd7uAnAFku+VWdb3reSmRV8PKSLfZ0Gjn3a6Xlcq9YGb6tADjn+lUfTXtVmaZ1KwBIvFI11rRXlWlatwIAAv2asaa9mlB9wwygiDX26qaw1yYPzFXg2N1GgG0FMF8Oj+VIx7E/03lHx8UhvYyNZLN7BwSPgekXXLribw7w5/c8EF+DBK5idvDVYtEEwMeYefjjLAdEyQ3M9nfOkgnPTEkYU+sxMq0BxNR6jExrAI31H1rzvLEfRIdgcv1XEdj6QTQAS2wtstEALLG1yEZ3QhH6oDX7ExBSFEkFINXH98NTrme5IOaaA7kIfiu2L8A3qhH9zRbukdCqdsA98TdElyeMe5BI8Rs2xHRIsoTSSVFfCFCWGPn9XHb4cdobRIWABNf0add9jakDjQJpJ1bTXOJXnnRXHRf+dNL1ZV1MBRCXhMbaHqGI1JkKIL7+i8uffuP6wVQAzO7+qVEbF6NbS0LJureYcWXUUhH66nLR5rYmva+2tjRFtojkM2aD76HEGAD3tPtKM309FJg5j/K682ywcWJ3PASCcycH/22u+Bh7Aa0ehM2Fu4z0SAE81HF9RkB21c5bEn4Dzw+/qNOyXr3DCTQDMBOdhi4nAgiFDGCinIa2owCEChUwD8qzd03PG+qdW/4fDzjUMcE1ZpIAAAAASUVORK5CYII=');
  }

  div.callout-caution.callout-style-default .callout-title {
    background-color: #ffe5d0
  }

  </style>
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Machine Learning and Data Mining (Module 2)</h1>
  <p class="subtitle">Modeling</p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Matteo Francia <br> DISI — University of Bologna <br> m.francia@unibo.it 
</div>
</div>
</div>

</section>
<section class="slide level2">

<p>DATA ACQUISITION</p>
</section>
<section id="machine-learning-tasks" class="title-slide slide level1 center">
<h1>3.1 Machine Learning Tasks</h1>

</section>

<section id="classification" class="title-slide slide level1 center">
<h1>Classification</h1>
<p>Regression Clustering</p>
</section>

<section id="machine-learning-tasks-1" class="title-slide slide level1 center">
<h1>Machine Learning tasks</h1>
<ul>
<li>There are different tasks in ML → depending on the output that we want:
<ul>
<li>Classification , regression and clustering</li>
</ul></li>
<li>Classification :
<ul>
<li>Given a specific input, the model ( <em>classifier</em> ) outputs a class (pattern → class) .
<ul>
<li>If there are only 2 classes, we call the problem binary classification</li>
<li>If there are multiple classes (&gt;2), we call the problem multi-class classification</li>
</ul></li>
<li>What is a class (the output of the classification task)?
<ul>
<li>A data set having common properties</li>
<li>The concept of class is related with the concept of “label” previously introduced
<ul>
<li>Different labels can be grouped in a single class.</li>
</ul></li>
<li>The concept of class is semantic , since it strictly depends by the working context
<ul>
<li>Examples:
<ul>
<li>Classification of italian letters → 21 classes</li>
<li>Classification of italian/indian alphabets → 2 classes</li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li>→ In our exercises, we will tackle classification tasks.</li>
</ul>

<img data-src="./img/neuralnetworks/3%20Model2.jpg" class="r-stretch"></section>

<section id="machine-learning-tasks-classification" class="title-slide slide level1 center">
<h1>Machine Learning tasks: Classification</h1>
<p><img data-src="./img/neuralnetworks/3%20Model3.png"></p>
<ul>
<li>Spam Detection
<ul>
<li>Data: email texts</li>
<li>Classes/labels: yes/no (spam)</li>
</ul></li>
<li>Credit Card Fraud Detection
<ul>
<li>Data: list of the bank operations</li>
<li>Classes/labels: yes/no (fraud)</li>
</ul></li>
<li>Face Recognition
<ul>
<li>Data: images</li>
<li>Classes/labels: identity</li>
</ul></li>
<li>Medical diagnosys
<ul>
<li>Data: x-ray images</li>
<li>Classes/labels: benign/malignant (tumor)</li>
</ul></li>
</ul>
<p><img data-src="./img/neuralnetworks/3%20Model4.png"></p>
<p><img data-src="./img/neuralnetworks/3%20Model5.jpg"></p>
</section>

<section id="machine-learning-tasks-2" class="title-slide slide level1 center">
<h1>Machine Learning tasks</h1>
<ul>
<li>Regression :
<ul>
<li>Given a specific input, the model ( <em>regressor</em> ) outputs a continuous value (data → value )</li>
<li>You can see a regression task as a classification task with a (very) high number of classes.</li>
<li>Usually, regression models != classification models</li>
</ul></li>
<li>Examples of regression tasks:
<ul>
<li>Estimation of a person’s height based on weight</li>
<li>Estimation of the sale prices for apartments in the real estate market</li>
<li>Risk estimation for insurance companies</li>
<li>Energy prediction produced by a photovoltaic system</li>
<li>Health costs prediction models</li>
</ul></li>
</ul>
<p>How many modules?</p>
<p>Latitude, longitude</p>
<p>Orientation (w.r.t sun)</p>
<p>…</p>

<img data-src="./img/neuralnetworks/3%20Model6.jpg" class="r-stretch"></section>

<section id="machine-learning-tasks-regression" class="title-slide slide level1 center">
<h1>Machine Learning tasks: Regression</h1>
<p>You can see the growth percentiles as a tool to perform regression in order to predict, for instance, the future height or weight.</p>
<p>How much pasta should I put if I have 5 guests (2 adults and 3 children)?</p>
<p><img data-src="./img/neuralnetworks/3%20Model7.png"></p>
<p><img data-src="./img/neuralnetworks/3%20Model8.jpg"></p>
</section>

<section id="machine-learning-tasks-3" class="title-slide slide level1 center">
<h1>Machine Learning tasks</h1>
<p><img data-src="./img/neuralnetworks/3%20Model9.png"></p>
<ul>
<li>Clustering :
<ul>
<li>Identify groups ( <em>clusters</em> ) of data with similar characteristics .</li>
<li>Clustering is often applied in an unsupervised learning setting, in which the patterns are not labeled and/or the classes of the problem are not known in advance.</li>
<li>Usually, the unsupervised nature of the problem makes it more complex than classification.</li>
<li>Often, even the number of clusters is not known a priori.</li>
<li>The clusters identified can be used as classes.</li>
<li>Clustering examples:
<ul>
<li>Marketing → definition of user groups based on consumption</li>
<li>Genetics → grouping of individuals based on DNA analogies</li>
<li>Bioinformatics → partitioning of genes into groups with similar characteristics</li>
<li>Vision → unsupervised segmentation</li>
</ul></li>
</ul></li>
</ul>
<p><img data-src="./img/neuralnetworks/3%20Model10.png"></p>
</section>

<section id="machine-learning-tasks-clustering" class="title-slide slide level1 center">
<h1>Machine Learning tasks: Clustering</h1>

<img data-src="./img/neuralnetworks/3%20Model11.png" class="r-stretch"></section>

<section id="machine-learning-tasks-4" class="title-slide slide level1 center">
<h1>Machine Learning tasks</h1>
<ul>
<li>For the Artificial Vision domain, we can identify even more specific problems:
<ul>
<li>Classification : (as before)</li>
<li>Localization : classification + object position in the image (through a <em>bounding box</em> )</li>
<li>Detection : multiple object classification + localization (through more <em>bounding</em> _ boxes_ )</li>
<li>Segmentation : each pixel in the image is classified</li>
</ul></li>
</ul>

<img data-src="./img/neuralnetworks/3%20Model12.png" class="r-stretch"></section>

<section id="example-of-segmentation" class="title-slide slide level1 center">
<h1>Example of Segmentation</h1>

<img data-src="./img/neuralnetworks/3%20Model13.jpg" class="r-stretch"><p><a href="https://youtu.be/X8vSk8R_zTA">https://youtu.be/X8vSk8R_zTA</a></p>
</section>

<section id="classification-through-a-set-of-explicitly-programmed-instructions" class="title-slide slide level1 center">
<h1>3.2 Classification through a set of explicitly programmed instructions</h1>

</section>

<section id="pattern-recognition" class="title-slide slide level1 center">
<h1>Pattern Recognition</h1>
<p>Expert Systems</p>
</section>

<section id="definitions" class="title-slide slide level1 center">
<h1>Definitions</h1>
<p>Pattern Recognition : the discipline that studies the recognition of patterns (“data samples”) even with pre-programmed algorithms (not able to learn automatically).</p>
<p>No learning phase!</p>
<p>The “model” is a set of hand-crafted instructions ( <em>if-then-else</em> paradigm)</p>
<p>Actually, the term Pattern Recognition is generally used to refer AI algorithms</p>
<p>DATA ACQUISITION</p>
</section>

<section id="explicitly-programmed-instructions" class="title-slide slide level1 center">
<h1>Explicitly programmed instructions</h1>
<ul>
<li>Technique similar to the <em>Expert Systems </em> developed in the ’80s (→ AI History)
<ul>
<li>A first form of Artificial Intelligence.</li>
<li>The ability of a calculator to perform (simple) calculations on large amounts of data is exploited.</li>
</ul></li>
<li>The programmer develops a series of instructions to solve a specific problem:
<ul>
<li>These instructions are typically based on if-then-else statements</li>
<li>A strong prior knowledge of the problem is required
<ul>
<li>In our case*: how to understand only from coordinates if the shape is a square, rectangle, triangle or rhombus?</li>
</ul></li>
</ul></li>
<li>What are the tasks that can be faced with the <em>explicitly programmed instructions</em> ?
<ul>
<li>The conditions are stable and known a priori → constrained industrial environments</li>
<li>There are mathematical formulas to model the problem</li>
<li>The problem must be limited in dimensionality and “not too complex”</li>
<li>…</li>
</ul></li>
<li>With the <em>Euclid</em> _ _ dataset</li>
</ul>
</section>

<section id="examples" class="title-slide slide level1 center">
<h1>Examples</h1>
<p>Anomaly prediction with</p>
<p>explicitly programmed instructions</p>
<p>Fit / Unfit prediction with</p>
<p>explicitly programmed instructions</p>
<p><img data-src="./img/neuralnetworks/3%20Model14.png"></p>
<p><img data-src="./img/neuralnetworks/3%20Model15.png"></p>
<ul>
<li>We observe anomalies when the value:
<ul>
<li>Is greater than a given threshold.</li>
<li>We can define the threshold as the mean value of the last 3 peaks.</li>
<li>The rate of change of the function is too fast.</li>
<li>…</li>
</ul></li>
</ul>
</section>

<section id="explicitly-programmed-instructions-1" class="title-slide slide level1 center">
<h1>Explicitly programmed instructions</h1>
<ul>
<li>General considerations about PR:
<ul>
<li>It can achieve a high degree of success:
<ul>
<li>If a priori knowledge is adequate → In our case, we have geometrics knowledge</li>
<li>Dimensionality of the problem is limited → In our case, only few coordinates (6 - 8)</li>
<li>The test domain is similar to what was assumed when defining the instructions</li>
</ul></li>
<li>The developed solution will inevitably be (too) specific :
<ul>
<li>In our case: what happens if a new <em>trapezoid</em> class is introduced?</li>
</ul></li>
</ul></li>
<li>Technical considerations about PR:
<ul>
<li>There is not a real learning phase → no training data is needed, all data for val/test.</li>
<li>It is possible to understand why the developed system fails in classification.</li>
<li>If the problem becomes complicated, the programming time increases.</li>
<li>The code risks becoming unmanageable due to:
<ul>
<li>High complexity and number of innested statements</li>
<li>Length of the code</li>
<li>Too specific functions</li>
</ul></li>
</ul></li>
</ul>
<p><img data-src="./img/neuralnetworks/3%20Model16.png"></p>
<p><img data-src="./img/neuralnetworks/3%20Model17.png"></p>
</section>

<section id="limits-of-programmed-instructions" class="title-slide slide level1 center">
<h1>Limits of programmed instructions</h1>
<ul>
<li>Task: Face Detection → Is there a face in the image?
<ul>
<li>Image → high dimensionality (128 x 128 x 3 = ~50k values)</li>
<li>It is easy for humans to “define” what a face is, but for a computer?
<ul>
<li>What is an eye? A nose? A mouth?</li>
<li>Assuming we can define these facial elements, what rules can we write?</li>
<li><em>“A face consists in two eyes with under a nose and again under a mouth”</em></li>
</ul></li>
</ul></li>
</ul>
<p><img data-src="./img/neuralnetworks/3%20Model18.jpg"></p>
<p><img data-src="./img/neuralnetworks/3%20Model19.jpg"></p>
<p><img data-src="./img/neuralnetworks/3%20Model20.jpg"></p>
<p><img data-src="./img/neuralnetworks/3%20Model21.jpg"></p>
<p>Here maybe we can identify a face</p>
<p><img data-src="./img/neuralnetworks/3%20Model22.jpg"></p>
<p><img data-src="./img/neuralnetworks/3%20Model23.png"></p>
<p>Other high-complexity problems:</p>
<p>Classification of traffic light light</p>
<p>Classification of road signs</p>
<ul>
<li>In short, solving these problems with instructions is very complex, if not impossible</li>
<li>The level of generalization of the proposed solution would be very limited</li>
<li>It is necessary, when needed , to address the problems with other paradigms!
<ul>
<li>The machine has to learn by itself how to solve a problem!</li>
</ul></li>
</ul>
<p><img data-src="./img/neuralnetworks/3%20Model24.jpg"></p>
</section>

<section id="classification-with-machine-learning" class="title-slide slide level1 center">
<h1>3.3 Classification with Machine Learning</h1>

</section>

<section id="the-machine-learns-by-itself-how-to-solve-a-problem" class="title-slide slide level1 center">
<h1>The machine learns by itself how to solve a problem</h1>

</section>

<section id="how-can-machine-learn" class="title-slide slide level1 center">
<h1>How can machine learn?</h1>
<ul>
<li>A machine can learn :
<ul>
<li>From data!</li>
<li>For that reason, data is so relevant today.</li>
</ul></li>
<li>Also humans can learn from data :
<ul>
<li>Currently, we can program machines that <em>imitate</em> this way of learning of humas</li>
<li>Remember that humans learn in many different (and efficient) ways, we only (badly) imitate just one.</li>
</ul></li>
<li>“Learning from data” is similar to humans learning to play a musical instrument (humans → machines):
<ul>
<li>I must observe how a chord is created → annotated data</li>
<li>I have to repeat the chord several times → iterative learning process</li>
<li>Someone has to tell me if I’m repeating it well → loss function</li>
</ul></li>
</ul>
<p><img data-src="./img/neuralnetworks/3%20Model25.png"></p>
<ul>
<li>From a practical point of view, these are the required steps:
<ul>
<li>We get the annotated data (we are in the Supervised Learning setting)</li>
<li>We pre-process data , making them suitable for the algorithm we want to use</li>
<li>We iteratively train a classifier</li>
<li>We measure the performance of the implemented solution</li>
</ul></li>
<li>We are going to use the <em>scikit-learn</em> library, developed for ML</li>
<li>Now we just have to introduce (some) classifier models :
<ul>
<li>We will not study clustering (already seen at <em>Data Mining</em> )</li>
<li>You have already seen some classifiers at <em>Data Mining</em></li>
</ul></li>
<li>We will see how to measure system performance</li>
</ul>
<p><img data-src="./img/neuralnetworks/3%20Model26.png"></p>
</section>

<section id="support-vector-machines" class="title-slide slide level1 center">
<h1>3.3.1 Support Vector Machines</h1>

</section>

<section id="svms" class="title-slide slide level1 center">
<h1>SVMs</h1>

</section>

<section id="support-vector-machines-svms" class="title-slide slide level1 center">
<h1>Support Vector Machines (SVMs)</h1>
<ul>
<li>A supervised learning method used for classification , regression and outliers’ detection.
<ul>
<li>Effective in high dimensional spaces.</li>
<li>Still effective in cases where number of dimensions is greater than the number of samples.</li>
</ul></li>
<li>Key ideas of SVMs (linear case):
<ul>
<li>Classification → linearly separate patterns</li>
</ul></li>
</ul>
<p>Hyperplanes , they are a decision surface</p>
<p>Which separating hyperplane?</p>
<p>In general, lots of possible solutions for a,b,c (an infinite number!)</p>
<p>SVM finds an optimal solution!</p>
<ul>
<li>SVMs maximize the margin between support vectors</li>
<li>The decision function is fully specified by a (usually very small) subset of training samples, the support vectors.</li>
<li>This becomes a <em>Quadratic programming </em> problem that is easy to solve by standard methods
<ul>
<li><em>Lagrange</em> multipliers</li>
</ul></li>
<li>Support vectors , the data points that lie closest</li>
<li>to the decision surface (or hyperplane).
<ul>
<li>These are data points most difficult to classify</li>
<li>They directly influence the optimum location of the decision surface</li>
<li>They are the elements of the training set that would change the position of the dividing hyperplane if removed</li>
</ul></li>
</ul>
<p>If you are interested in mathematical details: <a href="https://web.mit.edu/6.034/wwwbob/svm-notes-long-08.pdf">https://web.mit.edu/6.034/wwwbob/svm-notes-long-08.pdf</a></p>
<ul>
<li>What if patterms are not linearly separable?
<ul>
<li>The idea is to gain linearly separation by mapping the data to a higher dimensional space</li>
<li>The mapping procedure is realized through a kernel function</li>
</ul></li>
<li>What if we have more than 2 classes?
<ul>
<li>One- Against -One : classifiers trained on all possible class couples; a pattern is tested by each classifier that asisgn a vote; the class with the majority of votes is taken ( <em>majority</em> _ vote rule_ ).</li>
<li>One- Against - All : one SVM trained for each class (in which the secon class is the set of the other classes). At testing time, the SVM that has the better margin decides the final class.</li>
</ul></li>
</ul>

<img data-src="./img/neuralnetworks/3%20Model27.png" class="r-stretch"><p><a href="https://en.wikipedia.org/wiki/Kernel_method">https://en.wikipedia.org/wiki/Kernel_method</a></p>
</section>

<section id="hands-on-svms" class="title-slide slide level1 center">
<h1>Hands on SVMs</h1>
<ul>
<li>Linear or non-linear kernel?
<ul>
<li>If the dimensionality of the space is very high ( <em>e.g.,</em> 5000 features), linear SVM is generally used</li>
<li>For low dimensionality ( <em>e.g.,</em> 20 features) the primary choice is non-linear SVM with RBF kernel</li>
<li>For medium dimensionality ( <em>e.g.,</em> 200 features) both types are generally tried</li>
<li>Remember, the hyperparameters are calibrated on a separate validation set, or through cross validation</li>
</ul></li>
</ul>

<img data-src="./img/neuralnetworks/3%20Model28.png" class="r-stretch"><p><a href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html">https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html</a></p>
</section>

<section id="decision-trees" class="title-slide slide level1 center">
<h1>3.3.2 Decision Trees</h1>

</section>

<section id="dts" class="title-slide slide level1 center">
<h1>DTs</h1>

</section>

<section id="decision-tree" class="title-slide slide level1 center">
<h1>Decision Tree</h1>
<p>Tree-like model to perform the final classification.</p>
<p>Decision trees are commonly used in <em>operational research</em> , specifically in <em>decision analysis</em> .</p>
<p><img data-src="./img/neuralnetworks/3%20Model29.png"></p>
<p>Try to define when a data point belongs to class 1 (blue) or class 2 (green)</p>
<p>Probably, you just evaluated a decision tree in your head</p>
<p><img data-src="./img/neuralnetworks/3%20Model30.png"></p>
<p><a href="https://victorzhou.com/blog/intro-to-random-forests/">https://victorzhou.com/blog/intro-to-random-forests/</a></p>
<ul>
<li>What happen if we add a class?
<ul>
<li>Our old decision tree doesn’t work so well anymore!</li>
<li>We need to add another decision node</li>
</ul></li>
</ul>
<p><img data-src="./img/neuralnetworks/3%20Model31.png"></p>
<p><img data-src="./img/neuralnetworks/3%20Model32.png"></p>
<p><em>Intermediate </em> <em>Nodes</em></p>
</section>

<section id="decision-tree-training" class="title-slide slide level1 center">
<h1>Decision Tree: training</h1>
<ul>
<li>The root node:
<ul>
<li>We want a decision node that makes a “good” split → separating different classes as much as possible .</li>
<li>But how do we quantify how good a split is? → we need a measure → <em>Gini index, entropy, information gain</em> , …</li>
<li>Different possible algorithms ( <em>ID3</em> , <em>C4.5</em> and <em>CART</em> ) that recursively evaluating different features and using at each node the feature that best splits the data.</li>
</ul></li>
<li>The second node:
<ul>
<li>Let’s (arbitrarily) go to the left branch</li>
<li>We’re now only using the data that would that “belong” to the left branch</li>
<li>We just do the same thing as in the root node!</li>
<li>We apply this procedure to all the other nodes</li>
</ul></li>
<li>The predicted category would be the category on the leaf node</li>
<li>When to stop the training?
<ul>
<li>When the selected measure is not further increased</li>
</ul></li>
</ul>
</section>

<section id="ensemble-methods" class="title-slide slide level1 center">
<h1>3.3.3 Ensemble Methods</h1>

</section>

<section id="bagging-random-forest" class="title-slide slide level1 center">
<h1>Bagging → Random Forest</h1>
<p>Boosting → Adaboost</p>
</section>

<section id="ensemble-methods-combine-linear-classifiers" class="title-slide slide level1 center">
<h1>Ensemble methods: combine linear classifiers</h1>
<ul>
<li>A multi-classifier is an approach where several classifiers are used together :
<ul>
<li>In parallel</li>
<li>In cascade</li>
</ul></li>
<li>It has been shown the use of combinations of classifiers can strongly improve performance
<ul>
<li>In practice, investing (a lot of) time in “extremely” optimizing a single classifier is generally not convenient.</li>
</ul></li>
<li>The combination is effective only when individual classifiers are (at least partially) independent :
<ul>
<li>Using different features, different algorithms for feature extraction, different classification algorithms, …</li>
<li>Unfortunately, it is very difficult to have real independence between classifiers</li>
</ul></li>
<li>Two approaches:
<ul>
<li>Bagging : I train the same classification algorithm on different portions of the training set</li>
<li>Boosting : I focus the training on incorrectly classified patterns</li>
</ul></li>
</ul>
</section>

<section id="bagging-vs-boosting" class="title-slide slide level1 center">
<h1>Bagging vs Boosting</h1>

<img data-src="./img/neuralnetworks/3%20Model33.png" class="r-stretch"><p>random sampling with replacement</p>
<p>Image credits: <a href="https://pub.towardsai.net/bagging-vs-boosting-the-power-of-ensemble-methods-in-machine-learning-6404e33524e6">https://pub.towardsai.net/bagging-vs-boosting-the-power-of-ensemble-methods-in-machine-learning-6404e33524e6</a></p>
</section>

<section id="ensemble-methods-1" class="title-slide slide level1 center">
<h1>Ensemble methods</h1>
<p><img data-src="./img/neuralnetworks/3%20Model34.png"></p>
<ul>
<li>How to merge decisions of the individual classifiers :</li>
<li>Decision level
<ul>
<li>Majority vote rule
<ul>
<li>Each classifier votes for a class.</li>
<li>The pattern is assigned to the highest rated class.</li>
</ul></li>
<li>Borda count
<ul>
<li>Each classifier produces a ranking of the classes</li>
<li>The rankings are converted into scores .</li>
<li>The class with the highest final score is the one chosen.</li>
</ul></li>
</ul></li>
</ul>
<p><img data-src="./img/neuralnetworks/3%20Model35.png"></p>
<p><a href="https://en.wikipedia.org/wiki/Borda_count">https://en.wikipedia.org/wiki/Borda_count</a></p>
<ul>
<li>Confidence level
<ul>
<li>Each classifier outputs a confidence value , and these values are merged with different function: <em>sum</em> , <em>product</em> , <em>min</em> , <em>max</em> , …</li>
<li>Weighted sum : the sum of the confidence values is performed by weighing the different classifiers according to their degree of skill.</li>
</ul></li>
<li>NB : in practice, the <em>sum</em> is often preferable to the product as it is more robust .
<ul>
<li>Indeed, in the <em>product</em> it is sufficient that a single classifier indicates zero confidence to bring the confidence of the multi classifier to zero!</li>
</ul></li>
</ul>
<p><img data-src="./img/neuralnetworks/3%20Model36.jpg"></p>
</section>

<section id="random-forest" class="title-slide slide level1 center">
<h1>Random Forest</h1>
<ul>
<li>Proposed in 2001, based on bagging</li>
<li>The single classifier is a Decision Tree (DT):
<ul>
<li>Hundreds or even thousands of DTs for each forest</li>
</ul></li>
<li>2 types of bagging:
<ul>
<li>Data bagging : RF repeatedly selects a random sample with replacement of the training set and fits trees to these samples.</li>
<li>Feature bagging : in each decision node, the choice of the best feature on which to partition is not made on the entire set of 𝑑 feature (dimensionality of the patterns), but on a random subset.</li>
</ul></li>
</ul>

<img data-src="./img/neuralnetworks/3%20Model37.png" class="r-stretch"></section>

<section id="adaboost" class="title-slide slide level1 center">
<h1>Adaboost</h1>
<p>*Another version of the algorithm is with data sampling (if wrongly classified) with replacement</p>
</section>

<section id="classification-with-machine-learning-and-feature-descriptors" class="title-slide slide level1 center">
<h1>3.4 Classification with Machine Learning and Feature Descriptors</h1>

</section>

<section id="histogram-of-oriented-gradients" class="title-slide slide level1 center">
<h1>Histogram of Oriented Gradients</h1>
<p>Local Binary Patterns</p>
</section>

<section id="feature-descriptors" class="title-slide slide level1 center">
<h1>Feature Descriptors</h1>
<ul>
<li>The learning phase is complex with high- dimensionality data as images</li>
<li>For instance, what if in input we have RGB images ?</li>
<li>A first solution could be to “unroll” each image and to use each pixel’s value as input value:
<ul>
<li>The input dimensionality explodes: 224 x 224 x 3 = ~150k values !</li>
<li>The classifier receives too many values, it is not trivial understand what is important and what no</li>
</ul></li>
<li>A second solution is to introduce a new step in the machine learning paradigm:</li>
</ul>
<p><em>Human </em></p>
<p><em>intervention</em></p>
<p><img data-src="./img/neuralnetworks/3%20Model38.jpg"></p>
<p>0,25</p>
<p>0,366</p>
<p>1,25</p>
<p>…</p>
<p><img data-src="./img/neuralnetworks/3%20Model39.png"></p>
<p>High dimensional data</p>
<p>(image)</p>
<p>How to extract useful information in a “compact” way?</p>
<p>Feature Descriptor</p>
<p>(FD)</p>
</section>

<section id="feature-extraction" class="title-slide slide level1 center">
<h1>Feature Extraction</h1>
<p>Feature Extraction : the extraction procedure of features from data</p>
<p>Feature/Feature Descriptor : an <em>n</em> -dimensional vector of numerical features that represent (in a <em>discriminative </em> way) some object (used as input data).</p>
<p>Actually, the terms <em>feature</em> and <em>data</em> are often interchangeable</p>
<p>DATA ACQUISITION</p>
<p>FEATURE EXTRACTION</p>
</section>

<section id="definitions-1" class="title-slide slide level1 center">
<h1>Definitions</h1>
<ul>
<li>Data : the elements contained in a set ( <em>dataset</em> ) that we are going to use in our algorithm
<ul>
<li>Each element can named as <em>data point</em> , <em>observation</em> , <em>data sample, data</em> .</li>
</ul></li>
<li>Feature : individual characteristics or attributes used to describe and represent data points.
<ul>
<li>Also, the variables or properties that capture information about the data</li>
<li>The number of features is referred as dimensionality (!= the number of data)
<ul>
<li>If the dimensionality is 2, it is easy to plot and visualize the feature distribution</li>
</ul></li>
<li>Examples:
<ul>
<li>In a dataset of housing prices, features could include the number of bedrooms, location, and number of bathrooms.</li>
<li>In a dataset of geometric shapes, features include the coordinates of the vertices of each shape</li>
<li>In a dataset of people, features could include only the height and the weight of each person.</li>
</ul></li>
</ul></li>
</ul>

<img data-src="./img/neuralnetworks/3%20Model40.png" class="r-stretch"></section>

<section id="example-of-feature-descriptor" class="title-slide slide level1 center">
<h1>Example of Feature Descriptor</h1>
<ul>
<li>Object A geometric shape</li>
<li>Data Array of values (coordinates, diagonals, …)</li>
<li>Feature Descriptor
<ul>
<li>A (sub)set of the coordinates</li>
<li>A «new value» that we can compute from coordinates</li>
<li>…</li>
</ul></li>
<li>Object An image</li>
<li>Data Matrix of values (pixels)</li>
<li>Feature Descriptor
<ul>
<li>Unrolled or a subset of pixels</li>
<li>A «new value» that we can compute from pixels</li>
<li>…</li>
</ul></li>
</ul>
</section>

<section id="histogram-of-oriented-gradients-hog" class="title-slide slide level1 center">
<h1>Histogram of Oriented Gradients (HOG)</h1>
<ul>
<li>A visual feature descriptor (work with images)</li>
<li>Good to describe the shape of an object!</li>
<li>HOG provides the edge direction:
<ul>
<li>The whole image is divided into smaller regions</li>
<li>For each region, the edge directions are calculated
<ul>
<li>Edge → curves at which the brightness changes sharply</li>
<li>Direction → angle and magnitude of edges</li>
</ul></li>
<li>All these directions are provided through a histogram</li>
</ul></li>
</ul>
<p><img data-src="./img/neuralnetworks/3%20Model41.png"></p>
<p><img data-src="./img/neuralnetworks/3%20Model42.png"></p>
<p><img data-src="./img/neuralnetworks/3%20Model43.png"></p>
<p><img data-src="./img/neuralnetworks/3%20Model44.jpg"></p>
<p><img data-src="./img/neuralnetworks/3%20Model45.jpg"></p>
<p><a href="https://www.vlfeat.org/overview/hog.html">https://www.vlfeat.org/overview/hog.html</a></p>
<p>hog(img, orientations =8, pixels_per_cell =(4, 4), cells_per_block =(1, 1))</p>
<p><img data-src="./img/neuralnetworks/3%20Model46.png"></p>
</section>

<section id="local-binary-pattern-lbp" class="title-slide slide level1 center">
<h1>Local Binary Pattern (LBP)</h1>
<ul>
<li>A visual feature descriptor (work with images)</li>
<li>Good to describe the texture of the surfaces!</li>
<li>Texture → visual surface appearance
<ul>
<li>2 elements:
<ul>
<li>Local spatial patterns
<ul>
<li>The 3x3 neighborhood of each pixel is thresholded with the center value</li>
</ul></li>
<li>Gray scale contrast measure</li>
</ul></li>
<li>They are computed on different cells of the image</li>
<li>It has been extended to use neighborhoods of different sizes</li>
</ul></li>
</ul>
<p><img data-src="./img/neuralnetworks/3%20Model47.png"></p>
<p><img data-src="./img/neuralnetworks/3%20Model48.jpg"></p>
<p><img data-src="./img/neuralnetworks/3%20Model49.png"></p>
<ul>
<li>local_binary_pattern(img, P =100, R =5)
<ul>
<li>P: the number of points in a circularly symmetric neighborhood to consider.</li>
<li>R: the radius of the circle which allows us to account for different scales.</li>
</ul></li>
</ul>
<p><img data-src="./img/neuralnetworks/3%20Model50.jpg"></p>
<p><a href="https://pyimagesearch.com/2015/12/07/local-binary-patterns-with-python-opencv/">https://pyimagesearch.com/2015/12/07/local-binary-patterns-with-python-opencv/</a></p>
</section>

<section id="convolutional-neural-networks" class="title-slide slide level1 center">
<h1>3.5.1 Convolutional Neural Networks</h1>

</section>

<section id="cnns-the-real-core-of-the-current-ai-revolution" class="title-slide slide level1 center">
<h1>CNNs: the real «core» of the current AI revolution</h1>

</section>

<section id="convolutional-neural-networks-cnns" class="title-slide slide level1 center">
<h1>Convolutional Neural Networks (CNNs)</h1>
<ul>
<li>CNNs are particular Neural Networks specifically designed to process images
<ul>
<li>MLPs don’t scale well to full images</li>
<li>Instead of flatting input 3D data (let’s define as 3D also 1 channel images → ex. 224x224x1)…</li>
<li><em>…what about reshaping neurons in 3D?</em></li>
</ul></li>
</ul>
<p><img data-src="./img/neuralnetworks/3%20Model78.jpg"></p>
<p>3D</p>
<p>Input image</p>
<p>(224x224x1)</p>
<p>3D</p>
<p>Neurons</p>
<p>(3x3x1)</p>
<ul>
<li>Unlike MLPs, CNN layers have neurons arranged in 3 dimensions:
<ul>
<li>Width (W)</li>
<li>Height (H)</li>
<li>Depth (C) → usually, greater than 1</li>
</ul></li>
<li>How is possible to “connect” the 3D input with the smaller <em>kernel</em> (the “3D neuron set”)?
<ul>
<li>Through a mathematical operation called convolution!</li>
<li>The convolution is applied with a sliding-window process</li>
</ul></li>
</ul>
<p><img data-src="./img/neuralnetworks/3%20Model79.jpg"></p>
<p><img data-src="./img/neuralnetworks/3%20Model80.jpg"></p>
<p><a href="https://en.wikipedia.org/wiki/Convolution">https://en.wikipedia.org/wiki/Convolution</a></p>
</section>

<section id="convolution-with-images" class="title-slide slide level1 center">
<h1>Convolution with images</h1>
<ul>
<li>The convolution is the core building block of convolutional neural networks.
<ul>
<li>Each kernel is convolved with the input volume thus producing a 2D feature map</li>
<li>Usually, depth &gt; 1 → more kernels → <em>one feature map</em> for each kernel is produced</li>
<li>The output volume is then made up by stacking all activation maps produced one on the top of the other</li>
</ul></li>
</ul>
<p><img data-src="./img/neuralnetworks/3%20Model81.gif"></p>
<p><img data-src="./img/neuralnetworks/3%20Model82.png"></p>
<p>2x0 + 2x0 + 0x01x1 + 2x-1 + 1x0</p>
<p>2x1 + 1x1 + 1x-1</p>
<p>Output</p>
<p>( <em>Feature </em> <em>Map</em>)</p>
<p><a href="https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks">https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks</a></p>
<ul>
<li>Now the learnable weights (of the MLP) are grouped in the kernel!
<ul>
<li>Each cell can be viewed as a single weight to learn</li>
</ul></li>
</ul>
<p><img data-src="./img/neuralnetworks/3%20Model83.png"></p>
<p><img data-src="./img/neuralnetworks/3%20Model84.png"></p>
<p>1 channel image (7x7x1)</p>
<p>2 kernels (also called filters)</p>
<p>1 bias for each kernel</p>
<p>2 feature maps produced</p>
<p>3 channel image (7x7x3)</p>
<p>2 kernels (also called filters) applied on each channel</p>
<p>1 bias for each kernel</p>
<p>2 feature maps produced</p>
<p><a href="https://cs231n.github.io/convolutional-networks/">https://cs231n.github.io/convolutional-networks/</a></p>
</section>

<section id="section" class="title-slide slide level1 center">
<h1></h1>
<p><img data-src="./img/neuralnetworks/3%20Model85.png"></p>
<p>Esempio di kernel che evidenzia i contorni di una immagine.</p>
<p><img data-src="./img/neuralnetworks/3%20Model86.png"></p>
<p>Esempio di kernel che produce un effetto embossed.</p>
<p><img data-src="./img/neuralnetworks/3%20Model87.png"></p>
<p>Esempi di kernel che produce un effetto blur</p>
</section>

<section id="cnn-architecture" class="title-slide slide level1 center">
<h1>CNN Architecture</h1>
<p>A “traditional” CNN is made up by a whole bunch of layers stacked one on the top of the other:</p>

<img data-src="./img/neuralnetworks/3%20Model88.png" class="r-stretch"><p>Fully Connected layers (MLP)</p>
<p>2D convolution</p>
<p>layers</p>
<p>Feature extractor</p>
</section>

<section id="convolutional-layer" class="title-slide slide level1 center">
<h1>Convolutional layer</h1>
<ul>
<li>Convolutional layers learn to extract various types of visual information in a hierarchical manner
<ul>
<li>In the layers close to the input, CNNs learn filters to extract “simple” visual information</li>
<li>In the layers placed in depth, the filters extract semantically complex visual information</li>
</ul></li>
<li>The interesting thing is that this mechanism seems similar to what happens in our brain, where the visual cortex processes information by different layers.</li>
</ul>
<p><img data-src="./img/neuralnetworks/3%20Model89.png"></p>
<p><img data-src="./img/neuralnetworks/3%20Model90.png"></p>
</section>

<section id="section-1" class="title-slide slide level1 center">
<h1></h1>

<img data-src="./img/neuralnetworks/3%20Model91.png" class="r-stretch"></section>

<section id="convolutional-layer-1" class="title-slide slide level1 center">
<h1>Convolutional layer</h1>
<p>Moreover, it has been experimentally verified that some learned filters are similar to the neurological stimuli of the animal brain!</p>
<p><img data-src="./img/neuralnetworks/3%20Model92.png"></p>
<p><img data-src="./img/neuralnetworks/3%20Model93.png"></p>
<p>In a similar way, CNNs learns filters that provide a high response ( <em>i.e.</em> numeric value) with oriented edges!</p>
<p><em>Hubel</em> and <em>Wiesel</em> discovered oriented edges cause high responses in one specific cell of the visual cortex. The edge must be in certain position and orientation.</p>
<p><img data-src="./img/neuralnetworks/3%20Model94.png"></p>
<p><img data-src="./img/neuralnetworks/3%20Model95.png"></p>
<p><img data-src="./img/neuralnetworks/3%20Model96.png"></p>
<p><img data-src="./img/neuralnetworks/3%20Model97.png"></p>
<p><img data-src="./img/neuralnetworks/3%20Model98.jpg"></p>
<p>High response on edges</p>
<p>Hubel, D.H., Wiesel, T.N.: <em>Receptive fields of single </em> <em>neurones</em> * in the cat’s * <em>striatecortex</em>. The Journal of physiology (1959)</p>
</section>

<section id="pooling-layer" class="title-slide slide level1 center">
<h1>Pooling layer</h1>
<ul>
<li>Pooling layers spatially subsample the input volume.</li>
<li>Pooling layers are widely used for a number of reasons:
<ul>
<li>Gain robustness to exact location of the features</li>
<li>Reduce computational (memory) cost</li>
<li>Help preventing overfitting</li>
<li>Increase receptive field of following layers</li>
</ul></li>
<li>Many different functions could be used: <em>max</em> (most used), <em>average</em>,…</li>
</ul>

<img data-src="./img/neuralnetworks/3%20Model99.wmf" class="r-stretch"></section>

<section id="other-layers" class="title-slide slide level1 center">
<h1>Other Layers</h1>
<ul>
<li>Activation layer → see <em>activation </em> <em>functions</em></li>
<li>Flatten layer
<ul>
<li>Usually exploited to connect the 3D Feature Extractor to the 1D classifier</li>
<li>In input, there is a set of 3D feature maps (for instance, 12x12x64)</li>
<li>The 2D input is «unrolled», to fit the 1D input of the first fully connected layer</li>
<li>12x12x64 → 9216</li>
</ul></li>
<li>It is possibile to use other techniques to join the Feature Extractor and the classifier, e.g.&nbsp;the <em>Global </em> <em>Average</em> * Pooling * <em>layer</em></li>
</ul>
<p><img data-src="./img/neuralnetworks/3%20Model100.png"></p>
<p><img data-src="./img/neuralnetworks/3%20Model101.png"></p>
</section>

<section id="cnn-architectures-the-layer-organization" class="title-slide slide level1 center">
<h1>CNN architectures: the layer organization</h1>
<ul>
<li>More complex CNN architectures have recently been demonstrated to perform better than the traditional conv → ReLU → pooling stack architecture.</li>
<li>These architectures usually feature different graph topologies and much more intricate connectivity structures</li>
<li>However, these advanced architectures are out of the scope of these lectures.</li>
<li>In our exercitations, we are going to use well-known architectures proposed in literature:
<ul>
<li>VGG1</li>
<li>ResNet2</li>
<li>AlexNet3 *…</li>
</ul></li>
<li>All these architectures can be used pre-trained (already trained on datasets like ImageNet)</li>
</ul>

<img data-src="./img/neuralnetworks/3%20Model102.png" class="r-stretch"><ol type="1">
<li><p><a href="https://arxiv.org/abs/1409.1556">https://arxiv.org/abs/1409.1556</a></p></li>
<li><p><a href="https://arxiv.org/abs/1512.03385">https://arxiv.org/abs/1512.03385</a></p></li>
<li><p><a href="https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html">https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html</a></p></li>
</ol>
</section>

<section id="different-ways-to-train-cnns" class="title-slide slide level1 center">
<h1>Different ways to train CNNs</h1>
<ul>
<li>From scratch
<ul>
<li>The network is trained from “0”, only with an initialization of the weights</li>
<li>Large amount of data training required</li>
<li>Usually long and complex training procedure</li>
</ul></li>
<li>Pre-trained
<ul>
<li>A previously trained network is used</li>
<li>The output is defined by the previous training</li>
<li>When trained on excellent datasets, excellent performance</li>
</ul></li>
<li>Fine- tuned
<ul>
<li>A previously trained network is used</li>
<li>I keep “freezed” (unchanged) the Feature Extractor</li>
<li>I made changes to the Classifier (e.g.&nbsp;change the number of classes)</li>
</ul></li>
</ul>
</section>

<section id="references" class="title-slide slide level1 center">
<h1>References</h1>
<div class="quarto-auto-generated-content">
<div class="footer footer-default">
<p>Matteo Francia - Machine Learning and Data Mining (Module 2) - A.Y. 2024/25</p>
</div>
</div>
</section>
    </div>
  </div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="05-modeling_files/libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="05-modeling_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="05-modeling_files/libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="05-modeling_files/libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="05-modeling_files/libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="05-modeling_files/libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="05-modeling_files/libs/revealjs/plugin/notes/notes.js"></script>
  <script src="05-modeling_files/libs/revealjs/plugin/search/search.js"></script>
  <script src="05-modeling_files/libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="05-modeling_files/libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': true,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: true,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'bottom-right',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1100,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
        const config = {
          allowHTML: true,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start',
        };
        if (contentFn) {
          config.content = contentFn;
        }
        if (onTriggerFn) {
          config.onTrigger = onTriggerFn;
        }
        if (onUntriggerFn) {
          config.onUntrigger = onUntriggerFn;
        }
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          return note.innerHTML;
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>